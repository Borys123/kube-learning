1. Verify binaries
cd k8s-binaries/
ls
  kube-apiserver
  kubectl
  kubelet
  version.txt
cat version.txt
  v1.20.4
VERSION=$(cat version.txt)
curl -LO "https://dl.k8s.io/$VERSION/bin/linux/amd64/kubectl.sha256"
curl -LO "https://dl.k8s.io/$VERSION/bin/linux/amd64/kubelet.sha256"
curl -LO "https://dl.k8s.io/$VERSION/bin/linux/amd64/kube-apiserver.sha256"
echo "$(<kubectl.sha256) kubectl" | sha256sum --check
  OK
echo "$(<kubectl.sha256) kubelet" | sha256sum --check
  FAILED
echo "$(<kubectl.sha256) kube-apiserver" | sha256sum --check
  OK

2. pods/log !!!

3. TLS Termination
openssl req -nodes -new -x509 -keyout accounts.key -out accounts.crt -subj "/CN=accounts.svc"
vi accounts-tls-certs-secret.yml 
  apiVersion: v1
  kind: Secret
  type: kubernetes.io/tls 
  metadata:
    name: accounts-tls-certs
    namespace: accounts 
  data:
    tls.crt: |
      falkdfhkjdashgfasjkdlgh
    tls.key: |
      faksjdhfjkahdffasdgasgg
vi accounts-tls-ingress.yml 
  apiVersion: networking.k8s.io/v1
  kind: Ingress 
  metadata:
    name: accounts-tls
    namespace: accounts 
  spec:
    tls:
    - hosts:
        - accounts.svc
      secretName: accounts-tls-certs
    rules:
    - host: accounts.svc
      http: 
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: accounts-svc
              port:
                number: 80

4. PodSecurityPolicy

ENABLE ADMISSION CONTROLLER

sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
  - command:
    - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy

CREATE PodSecurityPolicy

vi psp-no-privileged.yml
  apiVersion: policy/v1beta1
  kind: PodSecurityPolicy
  metadata:
    name: psp-no-privileged
  spec:
    privileged: false
    runAsUser:
      rule: RunAsAny
    fsGroup:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
    - configMap
    - downwardAPI
    - emptyDir
    - persistentVolumeClaim
    - secret
    - projected

CREATE RBAC

vi cr-use-psp-no-privileged.yml 
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: cr-use-psp-no-privileged
  rules:
  - apiGroups: ['policy']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames:
    - psp-no-privileged

vi rb-auth-sa-psp.yml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: rb-auth-sa-psp
    namespace: auth
  roleRef:
    kind: ClusterRole
    name: cr-use-psp-no-privileged
    apiGroup: rbac.authorization.k8s.io
  subjects:
  - kind: ServiceAccount
    name: auth-sa
    namespace: auth

5. Secrets

kubectl get secret db-pass -oyaml -n users
  copy password
echo fasjdlkfj= | base64 --decode > ./dbpass.txt
echo "TrustNo1" | base64
kubectl edit secret db-pass -n users
  paste in the pass
RECREATE THE POD NOW!!!

6. Secure Runtime Sandbox

INSTALL GVISOR

-- ON ALL NDOES --

curl -fsSL https://gvisor.dev/archive.key | sudo apt-key add -
sudo add-apt-repository "deb blabla release main"
sudo apt-get update
sudo apt-get install -y runsc

sudo vi /etc/containerd/config.toml
disabled_plugins = ["io.containerd.internal.v1.restart"]
[plugins]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runsc]
      runtime_type = "io.containerd.runsc.v1"
  [plugins."io.containerd.runtime.v1.linux"]
    shim_debug = true

-- ON CONTROLPLANE --

vi runsc-sandbox.yml
  apiVersion: node.k8s.io/v1
  kind: RuntimeClass
  metadata:
    name: runsc-sandbox
  handler: runsc 

Modify containers
  spec:
    runtimeClassName: runsc-sandbox

7. Dockerfile analysis

FROM nginx: 1.19.10
RUN apt-get update && apt-get install -y mathblasters //DELETE THIS
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]

FROM nginx:1.19.10
USER root
RUN useradd -ms /bin/bash nginxuser
USER root //USER nginxuser
ENTRYPOINT ...
EXPOSE 80
CMD ...

FROM busybox:1.33.1
COPY db_connect /usr/local/bin/
ENV db_password=hunter2 //PASS AS KUBE SECRET, DELETE THIS LINE
CMD ["db_connect", "--user admin", "--password "]

8. Kube yaml analysis 

image: latest //set to some version

hostNetwork: false

securityContext:
  privileged: false 

9. Trivy

kubectl get po -n questionablesoft
kubectl describe pod -n questionablesoft questionablesoft-api
  busybox:1.33.1
webserver
  nginx:1.19.10
web-admin
  nginx:1.14.2

trivy image busybox:1.33.1 > busybox:1.33.1.log

10. Automate image vulnerability scanning

CONFIGURE ADMISSION CONTROLLER

vi /etc/kubernetes/admission-control/admission-control.conf
  apiVersion: apiserver.config.k8s.io/v1
  kind: AdmissionConfiguration
  plugins:
  - name: ImagePolicyWebhook
    configuration:
      ImagePolicy:
        kubeConfigFile: /etc/kubernetes/admission-control/imagepolicy_backend.kubeconfig
        allowTTL: 50
        denyTTL: 50
        retryBackoff: 500
        defaultAllow: false 

EDIT THE ADMISSIONS CONTROLLER'S KUBECONFIG TO POINT TO THE BACKEND WEBHOOK

vi /etc/kubernetes/admission-control/imagepolicy_backend.kubeconfig

add
  server: https://acg.trivy.k8s.webhook:8090/scan

ENABLE ANY NECESSARY ADMISSION CONTROL plugins

vi /etc/kubernetes/manifests/kube-apiserver.yaml
- command:
  - kube-apiserver
    - --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook

11. Falco

CREATE A FALCO RULES FILE CONFIGURED TO SCAN THE CONTAINER 

vi nginx-rules.yml
  - rule: spawned_process_in_nginx_container
    desc: A process was spawned in the Nginx container. 
    condition: container.name = "nginx" and evt.type = execve
    output: "%evt.time,%proc.name,%user.uid,%container.id,%container.name,%container.image"
    priority: WARNING

RUN FALCO, SAVE REPORT TO A FILE

falco --list

sudo falco -r nginx-rules.yml -M 45 > falco-report.log

12. Check k8s pods for container immutability

kubectl get po -n dev -oyaml web-frontend

allowPrivilegeEscalation: false - immutable 
runAsUser: 0 - not immutable!

allowPrivilegeEscalation: false - immutable
readOnlyRootFilesystem: true - immutable 

allowPrivilegeEscalation: false - immutable 
readOnlyRootFilesystem: false - not immutable! 

13. Configure Audit Logging

IMPLEMENT AUDIT POLICY RULES 

apiVersion: audit.k8s.io/v1
kind: Policy 
rules:
# Log requests and response bodies for all changes to Namespaces. 
- level: RequestResponse
  resources:
  - group: ""
    resources: ["namespaces"]
# Log request bodies (but not response bodies) for changes to Pods and Services in the web Namespace.
- level: Request
  resources:
  - group: ""
    resources: ["pods","services"]
  namespaces: ["web"]
# Log metadata for all changes to Secrets.
- level: Metedata
  resources:
  - group: ""
    resources: ["secrets"]
# Create a catchall rule to log metadata for all other requests.
- level: Metadata

CONFIGURE AUDIT LOGGING IN KUBE-APISERVER

vi /etc/kubernetes/manifests/kube-apiserver.yaml 
  - command:
    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
    - --audit-log-path=/var/log/kubernetes/k8s-audit.log
    - --audit-log-maxage=60
    - --audit-log-maxbackup=1
  And configure volumes/volumeMounts.
