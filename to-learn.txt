1. Important commands for deployments:

kubectl scale
kubectl rollout status (what's going on with rolling update)
kubectl set image deployment/my-deployment nginx=nginx:1.19.0 --record
  ( -- record - records rolling deployment, easy to roll back )
kubectl rollout history deployment.v1.apps/my-deployment
kubectl rollout undo deployment.v1.apps/my-deployment --to-revision=1
  ( without --to-revision - one back )

2. Pod FQDN:

pod-ip-address.namespace-name.pod.cluster.local
192-168-10-100.default.pod.cluster.local

3. Remember about three dashes!

4. NetworkPolicy
By default pods are completely open.
Any network policy selects a pod - it becomes isolated.

Components:
podSelector - to which pods in namespace policy applies. Uses labels.

spec:
  podSelector:
    matchLabels:
      role: db

from - selects ingress traffic to be allowed

to - selects egress traffic to be allowed

spec:
  ingress:
    - from:
  egress:
    - to:

Examples:

spec:
  ingress:
    - from:
      - podSelector:
        matchLabels:
          app: db

---

spec:
  ingress:
    - from:
      - namespaceSelector:
        matchLabels:
          app: db

---

spec:
  ingress:
    - from:
      - ipBlock:
        cidr: 172.17.20.0/12

ports - one/more ports that will allow traffic

spec:
  ingress:
    - from:
      ports:
        - protocol: TCP
          port: 80

Traffic has to match pod/ns/cidr AND port.

4. NodeSelector/NodeName
NodeSelector uses labels, NodeName - exact node.

5. Services

Endpoints - 1 endpoint for each pod.
kubectl get endpoints svc-clusterip

port: 80 - service listens on this port
targetPort: 80 - service routes to this port on the pod
nodePort: 30080 - port on the node traffic is routed to

Services also get DNS names.
FQDN:
service-name.namespace-name.svc.cluster-domain.example (cluster.local most of the time)
If same namespace, just use: service-name

Create service in an imperative way:

kubectl expose pod messaging --port=6379 --name messaging-service

6. Ingress

external -> ingress -> Service
Needs an ingress controller installed (nginx probably)

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - http:
    paths:
    - path: /somepath
      pathType: Prefix
      backend:
        service:
          name: my-service
          port:
            number: 80

Or route to named ports:

In Service:
ports:
  - name: web (ingress will look at the service port, auto change)
    protocol: TCP
    port: 80
    targetPort: 8080

In Ingress:
backend:
  service:
    name: svc-clusterip
    port:
      name: web

7. volumes:

Course of action if using PVs:

1. Create Persistent Volume
2. Create Pers. Vol. Claim
3. In pod add Volume (PVC actually)
4. In container add a Volume Mount

Pod:

spec:
  containers:
  - name: busybox
    image: busybox
    volumeMounts:
    - name: my-volume
      mountPath: /
  volumes:
  - name: my-volume
    hostPath:
      path: /data

You can the mount same volume to many containers - great way of interaction.

Common volume types:
hostPath - directory on the node
emptyDir - temporary, dynamically created, useful for simply sharing data between containers on a pod
persistentVolumeClaim - PVC

Persistent volumes:

kind: PersistentVolume
apiVersion: v1
metadata:
  name: my-pv
spec:
  storageClassName: localdisk
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /var/output

persistentVolumeReclaimPolicy: Recycle
Retain - keeps all data. Manually clean and prepare to reuse
Delete - delete RESOURCE automatically (only in cloud storage)
Recycle - delete DATA in storage automatically, allow reuse

Storage Class:

apiVersion: storage.k8s.io/v1
kind: storageClass
metadata:
  name: slow
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true (default false)

Persistent Volume Claim:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: localdisk
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi

PVC automatically binds to a PV after creation if finds a fitting one.

In Pod:

volumes:
- name: pv-storage
  persistentVolumeClaim:
    claimName: my-pvc

You can easily resize PVCs if PV allows and StorageClass allows.

8. TROUBLESHOOTING:

1. Is kube-apiserver down:
  - kubectl might not work
  - Connection to the server was refused (if kubeconfig is set up correctly!)
  - make sure docker/containerd and kubelet are up on controlplane nodes
2. See node status:
  - check status of nodes kubectl get nodes
  - kubectl describe node (one which is not READY)
3. If node is down, maybe a service is down on the node:
  - see kubelet and docker/containerd
  - systemctl status/start/enable kubelet
4. Check system pods in a kubeadm cluster (kube-system namespace):
  - kubectl get po -n kube-system
  - describe the ones that are down

Cluster and node logs:
journalctl -u kubelet
journalctl -u docker/containerd

kubectl logs -n kube-system podname

Unhealthy pods?:
- kubectl get po
- kubectl describe po
- kubectl exec podname -c containername -- command

Interactive shell:
kubectl exec busybox --stdin --tty -- /bin/sh

Checking container logs:
kubectl logs podname -c containername

Networking troubleshooting:
1. Networking plugin/infrastructure
2. kube-proxy
3. DNS

nicolaka/netshoot image - networking exploration/troubleshooting tools
command: ['sh', '-c', 'while true; do sleep 5; done']
In netshoot:
  - curl
  - ping
  - nslookup
  - many more (look up github page)

KUBELET DOES NOT RUN AS A POD IN KUBEADM CLUSTER!!!

9. Command

kubectl exec (-it) <pod name> -c <container-name> -- <command>
kubectl exec -i -t my-pod --container main-app -- /bin/bash
kubectl run <pod name> --command -- <command>

Two ways of adding commands to a container:

command: ['sh', '-c', 'while true; do sleep 3600; done']
command:
  - sh
  - '-c'

10. kubectl api-resources

11. kubectl create sa my-serviceaccount -n default

12. kubectl top (first install metrics-server)

13. Probes

livenessProbe: / startupProbe: / readinessProbe:
  exec:
    command:
  initialDelaySeconds:
  periodSeconds:
  httpGet:
    path: /
	port: 80
  failureThreshold (for startupProbe)

14. ConfigMaps and Secrets:

ConfigMap:
data:
  key1: value1

Secret:
data:
  secretkey1: value1

Creating secret value:
echo -n "blabla" | base64

Using these in a pod:

env:
- name: CONFIGMAPVAR
  valueFrom:
    configMapKeyRef: / secretKeyRef:
	  name: my-configmap
	  key: key1

as volume:

volumeMounts:
- name: configmap-volume
  mountPath: /etc/config/configmap
  readOnly: true (!!!!!!)

volumes:
- name: configmap-volume
  configMap:
    name: my-configmap
- name: secret-volume
  secret:
    secretName: my-secret

------------------------

1. Install a cluster

a) containerd and kubernetes tools

cat << EOF | sudo tee /etc/modules-load.d/containerd.conf
> overlay
> br_netfilter
> EOF

sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
> net.bridge.bridge-nf-call-iptables = 1
> net.ipv4.ip_forward = 1
> net.bridge.bridge-nf-call-ip6tables = 1
> EOF

sudo apt-get update && sudo apt-get install -y containerd

sudo mkdir -p /etc/containerd

sudo containerd config default | sudo tee /etc/containerd/config.toml

sudo systemctl restart containerd

sudo swapoff -a

sudo apt-get install -y apt-transport-https curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 
> deb https://apt.kubernetes.io/ kubernetes-xenial main
> EOF

sudo apt-get update
sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00
sudo apt-mark hold kubelet kubeadm kubectl

-- SAME ON WORKER NODES --

b) Initialize the cluster

sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0

COPY FROM OUTPUT:
mkdir -p $HOME/.kube
sudo cp -i /etc.kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

c) CNI (Calico)

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yml 

d) Join worker nodes

Get the join command from the controlplane:
kubeadm token create --print-join-command

Run copied command on both worker nodes as root 


2. Upgrade a cluster

-- controlplane --

FIRST CHECK FOR TAINS AND HEALTH!!!
kubectl drain controlplane --ignore-daemon-sets

apt update
apt install kubeadm=1.24.0-00 kubectl=1.24.0-00

kubeadm upgrade plan
kubeadm upgrade apply v1.24.0

apt install kubelet=1.24.0-00
systemctl daemon-reload
systemctl restart kubelet

kubectl uncordon controlplane

-- worker node --

FIRST CHECK FOR TAINS AND HEALTH!!!
kubectl drain node01 --ignore-daemon-sets

ssh node01
apt install kubeadm=1.24.0-00 kubectl=1.24.0-00

kubeadm upgrade node

apt install kubelet=1.24.0-00
systemctl daemon-reload
systemctl restart kubelet

ssh controlplane
kubectl uncordon node01

3. Backup and restore ETCD

Describe command reveals the configuration of the etcd service.
Look for the value of the option --listen-client-urls for the endpoint URL.

Example backup command:

ETCDCTL_API=3 etcdctl \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/etcd-backup.db

Example restore command:

ETCDCTL_API=3 etcdctl --data-dir=/var/lib/from-backup snapshot restore /opt/etcd-backup.db

vi /etc/kubernetes/manifests/etcd.yaml :
	- hostPath:
	  path: /var/lib/from-backup


4. Custom columns

kubectl get nodes -o=custom-columns=<COLUMN_NAME>:<JSON_PATH>,<>:<>

5. View controlplane config

kubectl config view
kubectl config view --kubeconfig=/....
https://controlplane:6443
