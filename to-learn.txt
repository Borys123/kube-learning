kubectl config use-context cluster1

1. Important commands for deployments:

kubectl rollout status (what's going on with rolling update)

kubectl rollout history deployment.v1.apps/my-deployment

kubectl rollout undo deployment.v1.apps/my-deployment --to-revision=1
  ( without --to-revision - one back )

kubectl set image deployment/my-deployment nginx=nginx:1.19.0 --record
  ( -- record - records rolling deployment, easy to roll back )

2. Pod and svc FQDN:

192-168-10-100.default.pod.cluster.local
name.default.svc.cluster.local


3. NetworkPolicy
By default pods are completely open.
Any network policy selects a pod - it becomes isolated.

Components:
podSelector - to which pods in namespace policy applies. Uses labels. podSelector: {} !!!

spec:
  podSelector:
    matchLabels:
      role: db

from - selects ingress traffic to be allowed

to - selects egress traffic to be allowed

spec:
  ingress:
    - from:
  egress:
    - to:

Examples:

spec:
  ingress:
    - from:
      - podSelector:
        matchLabels:
          app: db
      - namespaceSelector:
        matchLabels:
          ns: ns1
      - ipBlock:
          cidr: 172.17.20.0/12

spec:
  ingress:
    - from:
      ports:
        - protocol: TCP
          port: 80

4. NodeSelector/NodeName
NodeSelector uses labels, NodeName - exact node.



6. Ingress

external -> ingress -> Service
Needs an ingress controller installed (nginx probably)

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - http:
    paths:
    - path: /somepath
      pathType: Prefix
      backend:
        service:
          name: my-service
          port:
            number: 80

Or route to named ports:

In Service:
ports:
  - name: web (ingress will look at the service port, auto change)
    protocol: TCP
    port: 80
    targetPort: 8080

In Ingress:
backend:
  service:
    name: svc-clusterip
    port:
      name: web

7. volumes:

Course of action if using PVs:

1. Create Persistent Volume
2. Create Pers. Vol. Claim
3. In pod add Volume (PVC actually)
4. In container add a Volume Mount

Pod:

spec:
  containers:
  - name: busybox
    image: busybox
    volumeMounts:
    - name: my-volume
      mountPath: /
  volumes:
  - name: my-volume
    hostPath:
      path: /data

You can the mount same volume to many containers - great way of interaction.

Common volume types:
hostPath - directory on the node
emptyDir - temporary, dynamically created, useful for simply sharing data between containers on a pod
persistentVolumeClaim - PVC

Persistent volumes:

kind: PersistentVolume
apiVersion: v1
metadata:
  name: my-pv
spec:
  storageClassName: localdisk
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /var/output

persistentVolumeReclaimPolicy: Recycle
Retain - keeps all data. Manually clean and prepare to reuse
Delete - delete RESOURCE automatically (only in cloud storage)
Recycle - delete DATA in storage automatically, allow reuse

Storage Class:

apiVersion: storage.k8s.io/v1
kind: storageClass
metadata:
  name: slow
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true (default false)

Persistent Volume Claim:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: localdisk
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi

PVC automatically binds to a PV after creation if finds a fitting one.

In Pod:

volumes:
- name: pv-storage
  persistentVolumeClaim:
    claimName: my-pvc

You can easily resize PVCs if PV allows and StorageClass allows.

8. TROUBLESHOOTING:

1. Is kube-apiserver down:
  - kubectl might not work
  - Connection to the server was refused (if kubeconfig is set up correctly!)
  - make sure docker/containerd and kubelet are up on controlplane nodes
2. See node status:
  - check status of nodes kubectl get nodes
  - kubectl describe node (one which is not READY)
3. If node is down, maybe a service is down on the node:
  - see kubelet and docker/containerd
  - systemctl status/start/enable kubelet
4. Check system pods in a kubeadm cluster (kube-system namespace):
  - kubectl get po -n kube-system
  - describe the ones that are down

Cluster and node logs:
journalctl -u kubelet
journalctl -u docker/containerd

kubectl logs -n kube-system podname

Unhealthy pods?:
- kubectl get po
- kubectl describe po
- kubectl exec podname -c containername -- command

Interactive shell:
kubectl exec busybox --stdin --tty -- /bin/sh

Checking container logs:
kubectl logs podname -c containername

Networking troubleshooting:
1. Networking plugin/infrastructure
2. kube-proxy
3. DNS

nicolaka/netshoot image - networking exploration/troubleshooting tools
command: ['sh', '-c', 'while true; do sleep 5; done']
In netshoot:
  - curl
  - ping
  - nslookup
  - many more (look up github page)

KUBELET DOES NOT RUN AS A POD IN KUBEADM CLUSTER!!!

9. Command

kubectl exec (-it) <pod name> -c <container-name> -- <command>
kubectl exec -i -t my-pod --container main-app -- /bin/bash
kubectl run <pod name> --command -- <command>

Two ways of adding commands to a container:

command: ['sh', '-c', 'while true; do sleep 3600; done']
command:
  - sh
  - '-c'

10. kubectl api-resources

11. kubectl create sa my-serviceaccount -n default

12. kubectl top (first install metrics-server)

13. Probes

livenessProbe: / startupProbe: / readinessProbe:
  exec:
    command:
  initialDelaySeconds:
  periodSeconds:
  httpGet:
    path: /
	port: 80
  failureThreshold (for startupProbe)

14. ConfigMaps and Secrets:

ConfigMap:
data:
  key1: value1

Secret:
data:
  secretkey1: value1

Creating secret value:
echo -n "blabla" | base64

Using these in a pod:

env:
- name: CONFIGMAPVAR
  valueFrom:
    configMapKeyRef: / secretKeyRef:
	  name: my-configmap
	  key: key1

as volume:

volumeMounts:
- name: configmap-volume
  mountPath: /etc/config/configmap
  readOnly: true (!!!!!!)

volumes:
- name: configmap-volume
  configMap:
    name: my-configmap
- name: secret-volume
  secret:
    secretName: my-secret

------------------------

1. Install a cluster

a) containerd and kubernetes tools

cat << EOF | sudo tee /etc/modules-load.d/containerd.conf
> overlay
> br_netfilter
> EOF

sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
> net.bridge.bridge-nf-call-iptables = 1
> net.ipv4.ip_forward = 1
> net.bridge.bridge-nf-call-ip6tables = 1
> EOF

sudo apt-get update && sudo apt-get install -y containerd

sudo mkdir -p /etc/containerd

sudo containerd config default | sudo tee /etc/containerd/config.toml

sudo systemctl restart containerd

sudo swapoff -a

sudo apt-get install -y apt-transport-https curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 
> deb https://apt.kubernetes.io/ kubernetes-xenial main
> EOF

sudo apt-get update
sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00
sudo apt-mark hold kubelet kubeadm kubectl

-- SAME ON WORKER NODES --

b) Initialize the cluster

sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0

COPY FROM OUTPUT:
mkdir -p $HOME/.kube
sudo cp -i /etc.kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

c) CNI (Calico)

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yml 

d) Join worker nodes

Get the join command from the controlplane:
kubeadm token create --print-join-command

Run copied command on both worker nodes as root 


2. Upgrade a cluster

-- controlplane --

FIRST CHECK FOR TAINS AND HEALTH!!!
kubectl drain controlplane --ignore-daemon-sets

apt update
apt install kubeadm=1.24.0-00 kubectl=1.24.0-00

kubeadm upgrade plan
kubeadm upgrade apply v1.24.0

apt install kubelet=1.24.0-00
systemctl daemon-reload
systemctl restart kubelet

kubectl uncordon controlplane

-- worker node --

FIRST CHECK FOR TAINS AND HEALTH!!!
kubectl drain node01 --ignore-daemon-sets

ssh node01
apt install kubeadm=1.24.0-00 kubectl=1.24.0-00

kubeadm upgrade node

apt install kubelet=1.24.0-00
systemctl daemon-reload
systemctl restart kubelet

ssh controlplane
kubectl uncordon node01

3. Backup and restore ETCD

Describe command reveals the configuration of the etcd service.
Look for the value of the option --listen-client-urls for the endpoint URL.

Example backup command:

ETCDCTL_API=3 etcdctl \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--endpoints=https://etcd1:2379 \
snapshot save /opt/etcd-backup.db

Example restore command:

ETCDCTL_API=3 etcdctl --data-dir=/var/lib/from-backup snapshot restore /opt/etcd-backup.db

vi /etc/kubernetes/manifests/etcd.yaml :
	- hostPath:
	  path: /var/lib/from-backup


4. Custom columns, JSONPATH, sorting

kubectl get nodes -o=custom-columns=<COLUMN_NAME>:<JSON_PATH>,<>:<>
kubectl get persistentvolume --sort-by=.spec.capacity.storage


5. View controlplane config

kubectl config view
kubectl config view --kubeconfig=/....
https://controlplane:6443

6. Create a Role and RoleBinding

Try to get pods as user:
kubectl get pods -n beebox-mobile --kubeconfig /home/cloud_user/dev-k8s-config
or:
kubectl auth can-i get pods --as=dev --namespace=beebox-mobile

Create Role:
kubectl create role pod-reader --verb=get,watch,list --resource=pods,pods/log -n beebox-mobile

Create RoleBinding:
kubectl create rolebinding pod-reader-binding -n beebox-mobile --user=dev --role=pod-reader

7. Resource usage:
Need metrics-server to see resource usage. Wait a few minutes after installing.
kubectl top pod -n beebox-mobile --sort-by=cpu --selector app=auth

8. Secret and ConfigMap

htpasswd -c .htpasswd user
kubectl create secret generic nginx-htpasswd --from-file .htpasswd

In Pod:
spec:
  containers:
    volumeMounts:
    - name: config-volume
      mountPath: /etc/nginx
    - name: htpasswd-volume
      mountPath: /etc/nginx/conf
volumes:
- name: config-volume
  configMap:
    name: nginx-config
- name: htpasswd-volume:
  secret:
    secretName: nginx-htpasswd

9. Self-healing containers

restartPolicy + livenessProbe

spec:
  restartPolicy: Never - change to Always (Always, OnFailure and Never)
  livenessProbe:
    httpGet:
      path: /
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 5

10. command: ['sh', '-c', 'until nslookup shipping-svc; do echo waiting for shipping-svc; sleep 2; done']

11. Configure a pod to run on a specific node

kubectl label nodes k8s-worker2 external-auth-services=true
Edit Pod:
spec:
  nodeSelector:
    external-auth-services: "true"

12. Static pods: in /etc/kubernetes/manifests

13. "Starting kube-proxy" - probably no network plugin

14. Expose pods:
  1. Look up labels
  2. Look up containerPort
Service:
spec:
  type: ClusterIP
  selector:
    app: user-db
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

15. Ingress
  1. Create a service that maps to the deployment (ClusterIP) with port name
  spec:
    type: ClusterIP
    selector:
      app: web-auth
    ports:
      - name: http
        protocol: TCP
        port: 80
        targetPort: 80
  2. Create ingress
  spec:
    rules:
    - http:
      paths:
      - path: /auth
        pathType: Prefix
        backend:
          service:
            name: web-auth-svc
            port:
              number: 80

16. emptyDir: {}

17. ClusterRoleBinding in namespace:
kubectl create clusterrolebinding somename -n web --clusterrole=rolename --serviceaccount=web:serviceAccountName

18. remember kubernetes.io/no-provisioner - not in docs!!!

19. remember podSelector: {} in networkpolicy !!!

20. remember tail -f 

21.
securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]

In Pod OR Container!!!

22.
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
  - Ingress
  ingress:
  - ports: (NOTE THE - !!!!!)
    - protocol: TCP
      port: 80

ports - another way

ingress:
  - from:
    namespaceSelector:
      asdfagkjash
    ports:
      - protocol: TCP
        port: 80
    
23. crictl ps and crictl logs, /var/log/pods, /var/log/containers

24. PORTS: APISERVER - 6443, ETCD - 2379

25. kubectl create configmap --from-literal=key=value OR --from-file=

26. Remember ingressclass!!! And host!!! And namespace!!!

27. Can do clusterrole with rolebinding - works in one namespace. Also --serviceaccount/--user

---------------------
1.
kubectl get-contexts -o name
kubectl config current-context
cat ~/.kube/config | grep current

2.
tolerations:                                 # add
  - effect: NoSchedule                         # add
    key: node-role.kubernetes.io/control-plane # add
  nodeSelector:                                # add
    node-role.kubernetes.io/control-plane: ""

3.
kubectl get pod -A --sort-by=.metadata.creationTimestamp
kubectl get pod -A --sort-by=.metadata.uid

4.
      affinity:                                             # add
        podAntiAffinity:                                    # add
          requiredDuringSchedulingIgnoredDuringExecution:   # add
          - labelSelector:                                  # add
              matchExpressions:                             # add
              - key: id                                     # add
                operator: In                                # add
                values:                                     # add
                - very-important                            # add
            topologyKey: kubernetes.io/hostname             # add
Specify a topologyKey, which is a pre-populated Kubernetes label, you can find this by describing a node

5.
    env:                                                                          # add
    - name: MY_NODE_NAME                                                          # add
      valueFrom:                                                                  # add
        fieldRef:                                                                 # add
          fieldPath: spec.nodeName

6.
find /etc/cni/net.d/
The suffix is the node hostname with a leading hyphen

7.
kubectl get events -A --sort-by=.metadata.creationTimestamp

8.
kubectl api-resources --namespaced -o name
kubectl get role --no-headers

9.
/etc/systemd/system/kubelet.service.d/10-kubeadm.conf

10.
kubectl create secret --from-literal=user=aa7f3 --from-literal=pass=faf3j3

11.
kubeadm token create --print-join-command
kubeadm reset

12. 
openssl x509  -noout -text -in /etc/kubernetes/pki/apiserver.crt
kubeadm certs renew apiserver

13.
/var/lib/kubelet/pki/kubelet-client-current.pem
/var/lib/kubelet/pki/kubelet.crt (server cert)

14.


---------------------

kubectl scale

Remember ---

5. Services

Endpoints - 1 endpoint for each pod.
kubectl get endpoints svc-clusterip

port: 80 - service listens on this port
targetPort: 80 - service routes to this port on the pod
nodePort: 30080 - port on the node traffic is routed to

Create service in an imperative way:

kubectl expose pod messaging --port=6379 --name messaging-service