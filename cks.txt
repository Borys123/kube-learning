1. Secure an Ingress:

Ingress docs, scroll to TLS.
Create a secret with our cert and key.
in Ingress:

spec:
  tls:
  - hosts:
    - https-example.foo.com
    secretName: testsecret-tls
  - host: https-example.foo.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: service1
            port:
              number: 80

k create secret tls secure-ingress --cert=cert.pem --key=key.pem

2. kube-bench

kube-bench run --targets master
kube-bench run --targets master --check 1.2.20


3. sha512

sha512sum filename > compare
paste second line
cat compare | uniq

4. binaries
tar xzf binary
kubernetes/server/bin/kube-apivserver --version
sha512sum kubernetes/server/bin/kube-apivserver
crictl ps | grep api
ps aux | grep kube-apivserver - see pid
ls /proc/1843/root/ - root fs of the container
find /proc/1843/root/ | grep kube-apivserver
shasum it, append to compare filename

whereis kubelet

5. certs

COMMON NAME: 60099@internal.users

openssl x509 -req -in 60099.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out 60099.crt -days 500

cat jane.csr | base64 -w 0

k get csr jane -o yaml (copy certificate)
echo certificate | base64 -d > jane.crt

k config view
users:
-

k config set-credentials jane --client-key=jane.key --client-certificate=jane.crt 
or same with --embed-certs to include certs in kubeconfig

k config set-context jane --user=jane --cluster=kubernetes

k config get-contexts, k config use-context jane

k auth can-i

6. view and list

list secrets - also will access data with -o yaml!

7. sa

k create sa accessor
k create token accessor (pretty temporary)

copy token and paste in into a jwt inspector (jwt.io)

under pod spec:
serviceAccountName: 

token is mounted into a specific directory
mount | grep ser
will contain a token

kubectl -n one exec -it pod-one -- mount | grep serviceaccount

kubectl -n one exec -it pod-one -- cat /var/run/secrets/kubernetes.io/serviceaccount/token

env | grep KUBER 

curl https://10.96.0.1 -k (will identify as anonymous, error)

curl https://10.96.0.1 -k -H "Authorization: Bearer $(cat token)" (still forbidden, but we see auth as default accessor)

If we give RBAC to accessor, it will work.

8. Disabling SA mounting in pod 

Most often we don't need the pod to communicate with the API.

In SA:
automountServiceAccountToken: false

In Pod:
automountServiceAccountToken: false

9. Limit SAs using RBAC to edit resources

By default default sa doesn't really have permissions.
But if someone modifies it, it goes bad.
Better to use custom SAs.

We have a pod with accessor SA.

k auth can-i delete secret --as system:serviceaccount:default:accessor
can't

k create clusterrolebinding accessor --clusterrole edit --serviceaccount default:accessor

k auth ...
yes

10. Restrict API access

Authentication -> Authorization -> Admission Control

Restrictions:
- don't allow anonymous access
- close insecure port 
- don't expose ApiServer to the outside
- restrict access from Nodes to API (NodeRestriction)
- prevent unauthorized access (RBAC)
- prevent pods from accessing API
- Apiserver port behind firewall / allowed ip ranges (cloud provider)

11. Anonymous access 

kube-apiserver --anonymous-auth=true
Anonymous enabled by default, but:
- of auth mode other than Always allow
- but ABAC and RBAC require explicit auth for anonymous 

APIServer needs anonymous auth for liveness probes!

12. Insecure access

No longer possible since 1.20 (kube-apiserver --insecure-port 8080)

13. Manual API Request

k config view --raw = vim .kube/config

Extract certificate-authority-data 
echo ... | base64 -d -w 0 > ca
Same with client-certificate-data
echo ... | base64 -d -w 0 > crt 
client-key-data
echo ... | base64 -d -w 0 > key

k config view 
grab the server address : 6443

curl https://10.154.0.2:5443 --cacert ca
Works, authed as system:anonymous

curl https://10.154.0.2:5443 --cacert ca --cert crt --key=key
Authed as the administrator

14. External APIServer access

k edit svc kubernetes
edit as nodeport 

Remember need open firewall

k config view --raw
copy it to local machine conf file 
change server to external ip 
Cert is not valid for this IP.
openssl x509 -in /etc/kubernetes/pki/apiserver.crt
create a hosts entry in hosts file for the extrenal ip address as kubernetes 
in config file change to kubernetes too
Works!

15. NodeRestriction AdmissionController

kube-apiserver --enable-admission-plugins=NodeRestriction

Limits the Node labels a kubelet can modify
Can only modify certain labels (its only node labels, only labels of pods running on the same node)
Secure workload isolation via labels
No one can pretend to be a "secure" node and schedule "secure" pods

Verify it works:
vi /etc/kubernetes/manifests/kube-apiserver.yaml
--enable-admission-plugins=NodeRestriction (should be by default with kubeadm)
On worker node:
k config view
vi /etc/kubernetes/kubelet.conf - config for kubelet-apiserver communication
export KUBECONFIG=/etc/kubernetes/kubelet.conf - now its our kubectl config 
k get ns - no permissions (user system:node:nodename)
k get node - works 
k label node cks-master cks/test=yes - forbidden 
k label node cks-worker cks/test=yes - works 
Restricted labels we can't even set for ourselves
k label node cks-worker node-restriction.kubernetes.io/test=yes - not allowed to

16. Wrap up of auth

Outside -> API
Pod -> API
Node -> API

Anonymous access
Insecure access
Certificates

17. Secrets

ETCDCTL_API=3 etcdctl ...certs... get /registry/secrets/default/secret2

ETCD ENCRYPTION

Create an EncryptionConfiguration

apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - identity: {} kind of deafult provider, nothing is encrypted - plain text
    - aesgcm: encryption method 
        keys:
      - name: key1
        secret: adsfhljksadgh==
      - name: key2
        secret: adfhljkhdsf==
    - aescbc:
        keys:
        - name: key1
          secret: adfhlkjh==
        - name: key2
          secret: adksjfhd==

pass it as an argument to the APIServer
--encryption-provider-config - path to file ^

Encryption config (provider section) works in order!
First one is used on new resource save.

Might do resoruces: - secrets, first aesgcm, later identity as fallback

Need a provider to read! That's why leave identity: {} !!!

Encrypt all after change:
kubectl get secrets --all-namespaces -o json | kubectl replace -f -

Decrypt all: first set provider, second encrypted and recreate all like previously.

18. Container sandboxes

Sandboxes:
- more resources needed
- might be better for smaller containers
- not good for syscall heavy workloads

Contact kernel from the container:
k exec pod -it -- bash
uname -r 
on master the same
both these commands executed uname syscall, returned actual version

strace uname -r - will show which syscalls the process makes

Linux Kernel Dirty Cow 

OCI
kubelet --container-runtime {string}
kubelet --container-runtime-endpoint {string}

Kata containers

Additional isolation with a lightweight VM and individual kernels

- strong separation layer
- runs every container in its own private VM (hypervisor based)
- QEMU as default (needs virtualisation, like nested virtualisation in cloud - might not be asy to configur if already in a VM)

gVisor

user-space kernel for containers

- adds another layer of separation
- not hypervisor/VM based
- simulates kernel syscalls with limited functionality
- runs in userspace separated from Linux Kernel
- runtime called runsc

It's a kernel in golang that accepts syscalls and transforms them to the real kernel

Create and use RuntimeClasses for runsc (gvisor)

RuntimeClass is a k8s resource

apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc

in pod:

spec:
  runtimeClassName: gvisor

scp gvisor-install.sh node01:/root

ssh node01
    sh gvisor-install.sh
    service kubelet status

19. OS Level Security Domains

Security Contexts
Define privilege and access control for Pod/Container
- userID and groupID
- run privileged/unprivileged
- linux capabilities
- ...

spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    this is passed down to all containers
  containers:
    - name: asfafg
    securityContext:
      runAsUser: 0

command id for checking

Force container to run as non-root

vi pod.yaml

spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
  containers:
  -
    securityContext:
      runAsNonRoot: true - container is not allowed to run as root user

Privileged containers
- default containers run unprivileged
- possible to run as pribileged to:
  - access all devices
  - run docker daemon inside container
  - docker run --privileged
- means that container user 0 = host user 0

Enable privileged mode and test using sysctl (set kernel params at runtime if have privileges)

in pod:
enable security context again 1000,3000
run as runAsNonRoot

k exec -it pod -- sh
sysctl kernel.hostname=attacker - won't work 

in pod:
delete old stuff
containers:
  securityContext:
    privileged: true
hostname will work

PrivilegeEscalation

AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process

By default it's allowed.

securityContext:
  allowPrivilegeEscalation: false

/proc/1/status - NoNewPrivs = 1

Pod SecurityPolicies
- Cluster-level resources
- Control under which security conditions a pod has to run

We create a policy and with this policy we kind of set a base all the pods in the cluster have to follow.
Otherwise they won't be created. It's actually an AdmissionController, we enable it and all pods go through
this on creation.

PodSecurityPolicy controls what has been defined in the Pod (eg. SecurityContext)

What can we set:
privileged - running of privileged containers
hostPID, hostIPC - usage of host namespaces
hostNetwork, hostPorts - usage of host networking and ports 
volumes- usage of volume types
allowedHostPaths - usage of the host filesystem
allowedFlexVolumes - allow specific FlexVolume drivers
fsGroup - allocating an FSGroup that owns the pod's volumes
readOnlyRootFilesystem - requiring the use of a read only root filesystem

--enable-admission-plugins=PodSecurityPolicy

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - '*'

Deployment now doesn't work, but k creat pod does.
Because deployment creates the pod - deployment account doesn't see policy.

Fix serviceaccount

k create role psp-access --verb=use --resource=podsecuritypolicies 
k create rolebinding psp-access --role=psp-access --serviceaccount=default:default

Works now!

20. mTLS

- mutual authentication
- two-way (bilateral) authentication 
- two parties authenticating each other at the same time 

To proxy, create an initContainer that needs NET_ADMIN capability

k run app --image=bash --command -oyaml --dry-run=client > file.yml -- sh -c 'ping google.com'

containers:
- bash cont 
- name: proxy
  image: ubuntu
  command:
  - sh
  - c
  - 'apt-get update && apt-get install iptables -y && iptables -L && sleep 1d' (hacky)
  securityContext:
    capabilities:
      add: ["NET_ADMIN"]

21. OPA

General-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack
- not k8s specific
- rego language
- works with json/yaml
- in k8s uses AdmissionControllers
- doesn't know concepts like pods/deployments

OPA Gatekeeper provides K8s CRDs 

Constraint template: (search for necesary labels)
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: K8sRequiredLabels

Constraint: (pods must have labels X)
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: pod-must-have-gk 

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: ns-must-have-gk

Install OPA Gatekeeper

apiserver: --enable-admission-plugins=NodeRestriction (no others)
k create -f https://blablabal

In k8s docs: DYNAMIC ADMISSION CONTROL (admission webhooks)

OPA Gatekeeper creates custom webhooks - every pod creation passes through this webhooks.
There are 2 ways - validation admission webhook (only validate pod spec - apply/deny)
Mutating admission webhook - mutates a pod.
Gatekeeper works with a validating one.

Deny All - Approve All Policy 

k get crd (custom resource definitions)

k get constrainttemplates 

alwaysdeny_template.yml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8salwaysdeny
spec:
  crd:
    spec:
      names:
        kind: K8sAlwaysDeny 
      validation:
        #Schema for the 'parameters' field 
        openAPIV3Schema:
          properties:
            message:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8salwaysdeny

        violation[{"msg": msg}] {
          1 > 0
          msg := input.parameters.message
        }

all_pod_always_deny.yml 
apiVersion:constraints.gatekeeper.sh/v1beta1
kind: K8sAlwaysDeny
metadata:
  name: pod-always-deny 
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
  parameters:
    message: "ACCESS DENIED!"

k get K8sAlwaysDeny

k run --image=nginx - ACCESS DENIED!

k describe K8sAlwaysDeny pod-always-deny

Change it to approve all:

vi template
change 1 > 0 to 1 > 2

All NSes created need to have label "cks"

apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels 
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          properties:
            labels:
              type: array
              items: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        violation [{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("you must provide labels: %v, [missing]")
        }

all_pods_must_have_cks_label.yml

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: pod-must-have-cks
spec:
  math:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
  parameters:
    labels: ["cks"]

all_ns_must_have_cks_label.yml

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: ns-must-have-cks
spec:
  math:
    kinds:
      - apiGroups: [""]
        kinds: ["Namespace"]
  parameters:
    labels: ["cks"]

Enforce deployment replica count

apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sminreplicacount 
spec:
  crd:
    spec:
      names:
        kind: K8sMinReplicaCount 
      validation:
        openAPIV3Schema:
          properties:
            min:
              type: integer
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sminreplicacount
        
        violation[{"msg": msg, "details": {"missing_replicas": missing_replicas}}]
          provided := input.review.object.spec.replicas 
          required := input.parameters.min
          missing := required - provided 
          missing > 0
          msg := sprintf("you must provide %v more replicas", [missing_replicas])
        }

        apiVersion: constraints.gatekeeper.sh/v1beta1
        kind: K8sMinReplicaCount
        metadata:
          name: deployment-must-have-min-replicas
        spec:
          match:
            kinds:
              - apiGroups: ["apps"]
                kinds: ["Deployment"]
          parameters:
            min: 2

22. Image security

- use specific image versions

- don't run as root
(in alpine)
RUN addgroup -S appgroup && adduser -S appuser -G appgroup -h /home/appuser 
COPY --from=0 /app /home/appuser/
USER appuser
CMD ["/home/appuser/app]

- make filesystem RO 
(can also do in security context k8s)
RUN chmod a-w /etc

- remove shell access
RUN rm -rf /bin/* (in a good place though! After adduser)

- remove caching issues (&& line with apt-gets)

- no hardcoded secrets 
CMD ["sh", "-c", "curl --head $URL$TOKEN"]
podman run -e TOKEN=2e064aad-3a90-4cde-ad86-16fad1f8943e app

rm -rf /usr/bin/bash in Killercoda

23. SCS - static analysis of user workloads

Example rules:
- always define resource requests and limits 
- pods should never use the default serviceAccount
Generally: don't store sensitive data plain text in yaml/dockerfiles 

Kubesec 

docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin < pod.yaml

OPA Conftest 
OPA - Open Policy Agent 
Unit test framework for Kubernetes configurations 
Uses Rego lanugage 

Use conftest to check example k8s yaml 

vim policy/deployment.rego 

package main 
deny[msg] {
  input.kind = "Deployment"
  not input.spec.template.spec.securityContext.runAsNonRoot = true
  msg = "Containers must not run as root"
}

package main
deny[msg] {
  input.kind = "Deployment"
  not input.spec.selector.matchLabels.app
  msg = "Containers must provide app label for pod selectors"
}

docker run --rm -v $(pwd):/project instrumenta/conftest test deploy.yaml

Conftest for dockerfiles 

package main 

denylist = [
  "ubuntu"
]

deny[msg] {
  input[i].Cmd == "from"
  val := input[i].Value
  contains(val[i], denylist[_])

  msg = sprintf("unallowed image found %s, [val])
}

package commands 

denylist = [
  "apk",
  "apt",
  "pip",
  "curl",
  "wget",
]

deny[msg] {
  input[i].Cmd == "run"
  val := input[i].Value
  contains(val[_], denylist[_])

  msg = sprintf("unallowed comments found %s", [val])
}

24. Image scanning for known vulnerabilities

cve.mitre.org
nvd.nist.gov 

Clair:
- open source
- static analysis of vulnerabilities in application containers
- ingests vulnerability metadata from a configured set of sources
- provides API

Trivy:
- open source
- a simple and comprehensive vuln scanner for containers and others, compatible with CI 

Using Trivy to check some public images and kube-apiserver image 

docker run ghcr.io/aquasecurity/trivy:latest image nginx

k -n applications get pod -oyaml | grep image

or just trivy image imagename | grep CVE-...

25. Secure supply chain

docker login 

In kubernetes:
kubectl create secret docker-registry my-private-registry \
--docker-server=my-private-registry-server \
--docker-username=username \
--docker-password=password \
--docker-email=email 

kubectl patch serviceaccount defaut -p '{"imagePullSecrets": [{"name": "my-private-registry"}]}'

List all image registries used in the whole cluster 
Use Image digest for kube-apiserver 

k get pod -A -oyaml | grep "image:" | grep -v "f:"

k8s.gcr.io and docker.io None = docker.io

Tags can be overwritten.
See imageID under conatainerStatuses
There's the image digest (@sha256:adsfhjkghd)

In yaml replace image with the digest 

Whitelist docker.io and k8s.gcr.io with OPA

Install gatekeeper 

apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8strustedimages
spec:
  crd:
    spec:
      names:
        kind: K8sTrustedImages 
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8strustedimages

        violation [{"msg": msg}] {
          image := input.review.object.spec.containers[_].image
          not startswith(image, "docker.io/")
          not startswith(image, "k8s.gcr.io/")
          msg := "not trusted image!"
        }
    
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sTrustedImages
metadata:
  name: pod-trusted-images
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]

k run nginx --image=nginx (not trusted image - always have to specify the registry now!)

ImagePolicyWebhook

Example ImagePolicyWebhook

"apiVersion":"imagepolicy.k8s.io/v1alpha1",
"kind":"ImageReview",
"spec":{
  "containers":[
    {
      "image":"myrepo:myimage:v1"
    },
    {
      "image":"myrepo:myimage@sha256:asdldsjghl"
    }
  ],
  "annotations":{
    "mycluster.image-policy.k8s.io/ticket-1234":"break-glass"
  "namespace":"mynamespace"
  },
}

Investigate ImagePolicyWebhook and use it up to the point where it calls an external service 

(external service doesn't exist in our case, let's pretend it's down)
Best to remove OPA Gatekeeper now.

vi /etc/kubernetes/manifests/kube-apiserver.yaml
--enable-admission-plugins=NodeRestriction,ImagePolicyWebhook

ApiServer will be mad now. ImagePolicyWebhook - no config specified

Copy example from course repo to /etc/kubernetes/adminssion:
admission_config.yaml
certs 
kubeconf 

vi admission_config.yaml

apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
  - name: ImagePolicyWebhook
    configuration:
      imagePolicy:
        kubeConfigFile: /etc/kubernetes/admission/kubeconf
        allowTTL: 50
        denyTTL: 50
        retryBackoff: 500
        defaultAllow: false (all pod creation denied, even if ext service down)

kubeconf:
Clusters refers to the remote service 
users refers to the API server's webhook configuration 

vi kube-apiserver.yaml, add:
--admission-control-config-file=/etc/kubernetes/admission/admission_config.yaml 
It is on the machine, but not in the container. Configure volumes (hostpath).

k run test --image=nginx - forbidden (post to url no such host - external service down)

26. Behavioral Analytics at host and container level

Applications - Firefox, Curl | user space
Libraries - glibc, libxyz    |         
Syscall Interface - getpid(), reboot() | kernel space
Linux Kernel                           |
Hardware

Applications communicate with syscall interface usually through libraries, but can also directly.

seccomp/apparmor - lay between userspace and syscall interface
Filter syscalls etc 

strace
- intercepts and logs system calls made by a process
- log and display signals received by a process
- usually preinstalled
- diagnostic, learning, debugging

strace
 -o filename
 -v verbose
 -f follow forks

 -cw (count and summarise)
 -p pid
 -P path

strace ls /
execve - proper path was executed
access - access file 
openat - open a file 
fstat - get file status
close 

strace -cw ls / - nice output, numbers and not verbose

Strace and /proc on ETCD

/proc
- information and connection to processes and kernel
- study it to learn how processes work
- configuration and administrative tasks 
- contains files taht don't exist, yet you can access these (created on access)
- kind of communication interface with the Linux kernel

strace Kubernetes etcd 

1. List syscalls

crictl ps | grep etcd
ps aux | grep etcd 

strace -p 3502 (-f more, follow subprocesses/forks)

strace -p 3502 -f -cw (wait 3 secs, ctrl + c)

2. Find open files

cd /proc/3502
ls
ls -lh exe
cd fd
ls -lh - all open files and open sockets

tail 7 -f (was a database)
SOme binary output 

3. Read secret value 

k create secret generic creadit-card --from-literal cc=11223344

cat 7 | grep 11223344 - nothing, binary file 
cat 7 | strings | grep 11223344 - found!

Create Apache pod with a secret as environment variable
Read that secret from host filesystem

k run apache --image=httpd -oyaml --dry-run=client > pod.yaml
env:
- name: SECRET
  value: "1231254"

k exec apache -- env 
SECRET=1231254

ps aux | grep httpd (on worker node)
crictl ps | grep apache
pstree -p (output pid)
containerd -> containerd-shim -> httpd -> httpd -> httpd{...}
Copy PID of first httpd 

cd /proc/28696
cat environ - we have the secret 

Falco and Installation

Falco 
- Cloud-Native runtime security (CNCF)
- ACCESS
  * deep kernel tracing built on the Linux kernel (overview about processes, investigate them, kernel activity, syscalls)
- ASSERT
  * describe security rules against a system (+default ones)
  * detect unwanted behavior, maybe create some logs
- ACTION 
  * automated respond to a security violation

Install Falco on worker node 
Auditing with Falco in k8s docs 

curl -s <repo>
apt install -y falco 
service falco start 

cd /etc/falco/
vi falco.yaml

tail -f /var/log/syslog | grep falco

Use Falco to find malicious processes

k exec -it apache -- bash
Notice A shell was spawned...

echo "user" /etc/passwd 
Error File below /etc opened for writing...

Add some readiness probe to the container with apt-get update
Error Package management process launched...

Liveness probe fails - container restart 
Readiness probe fails - no new requests redirected to the container

Investigate Falco rules

cd /etc/falco
vi falco_rules.yaml
/was spawned (search)

vim k8s_audit_rules.yaml 
/etc

Rules and macros (macros can be called by conditions of rule sections)

Change Falco rule to get custom output format

Rule: "A shell was spawned in a container with an attached terminal"
Output Format: TIME,USER-NAME,CONTAINER-NAME,CONTAINER-ID 
Priority: WARNING

You can service falco stop and use falco from the commandline

cd /etc/falco/
grep -r "A shell was ......" - falco_rules.yaml
vi falco_rules.yaml

Copy the rule and paste in falco_rules.local  - override existing rules

Exchange notice with warning 
For TIME: falco.org/docs -> supported fields for conditions and outputs
output: >
  (%evt.time,%user.name,%container.name,%container.id)

27. Container immutability

- remove bash/shell
- fs r/o
- run as user and non root

What if we have no control of the container image?

Manual changes using --command

pod starts -> app container starts -> app container runs
Command: chmod a-w -R / && nginx
