1. Secure an Ingress:

Ingress docs, scroll to TLS.
Create a secret with our cert and key.
in Ingress:

spec:
  tls:
  - hosts:
    - https-example.foo.com
    secretName: testsecret-tls
  - host: https-example.foo.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: service1
            port:
              number: 80

k create secret tls secure-ingress --cert=cert.pem --key=key.pem

2. kube-bench

kube-bench run --targets master
kube-bench run --targets master --check 1.2.20


3. sha512

sha512sum filename > compare
paste second line
cat compare | uniq

4. binaries
tar xzf binary
kubernetes/server/bin/kube-apivserver --version
sha512sum kubernetes/server/bin/kube-apivserver
crictl ps | grep api
ps aux | grep kube-apivserver - see pid
ls /proc/1843/root/ - root fs of the container
find /proc/1843/root/ | grep kube-apivserver
shasum it, append to compare filename

whereis kubelet

5. certs

COMMON NAME: 60099@internal.users

openssl x509 -req -in 60099.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out 60099.crt -days 500

cat jane.csr | base64 -w 0

k get csr jane -o yaml (copy certificate)
echo certificate | base64 -d > jane.crt

k config view
users:
-

k config set-credentials jane --client-key=jane.key --client-certificate=jane.crt 
or same with --embed-certs to include certs in kubeconfig

k config set-context jane --user=jane --cluster=kubernetes

k config get-contexts, k config use-context jane

k auth can-i

6. view and list

list secrets - also will access data with -o yaml!

7. sa

k create sa accessor
k create token accessor (pretty temporary)

copy token and paste in into a jwt inspector (jwt.io)

under pod spec:
serviceAccountName: 

token is mounted into a specific directory
mount | grep ser
will contain a token

kubectl -n one exec -it pod-one -- mount | grep serviceaccount

kubectl -n one exec -it pod-one -- cat /var/run/secrets/kubernetes.io/serviceaccount/token

env | grep KUBER 

curl https://10.96.0.1 -k (will identify as anonymous, error)

curl https://10.96.0.1 -k -H "Authorization: Bearer $(cat token)" (still forbidden, but we see auth as default accessor)

If we give RBAC to accessor, it will work.

8. Disabling SA mounting in pod 

Most often we don't need the pod to communicate with the API.

In SA:
automountServiceAccountToken: false

In Pod:
automountServiceAccountToken: false

9. Limit SAs using RBAC to edit resources

By default default sa doesn't really have permissions.
But if someone modifies it, it goes bad.
Better to use custom SAs.

We have a pod with accessor SA.

k auth can-i delete secret --as system:serviceaccount:default:accessor
can't

k create clusterrolebinding accessor --clusterrole edit --serviceaccount default:accessor

k auth ...
yes

10. Restrict API access

Authentication -> Authorization -> Admission Control

Restrictions:
- don't allow anonymous access
- close insecure port 
- don't expose ApiServer to the outside
- restrict access from Nodes to API (NodeRestriction)
- prevent unauthorized access (RBAC)
- prevent pods from accessing API
- Apiserver port behind firewall / allowed ip ranges (cloud provider)

11. Anonymous access 

kube-apiserver --anonymous-auth=true
Anonymous enabled by default, but:
- of auth mode other than Always allow
- but ABAC and RBAC require explicit auth for anonymous 

APIServer needs anonymous auth for liveness probes!

12. Insecure access

No longer possible since 1.20 (kube-apiserver --insecure-port 8080)

13. Manual API Request

k config view --raw = vim .kube/config

Extract certificate-authority-data 
echo ... | base64 -d -w 0 > ca
Same with client-certificate-data
echo ... | base64 -d -w 0 > crt 
client-key-data
echo ... | base64 -d -w 0 > key

k config view 
grab the server address : 6443

curl https://10.154.0.2:5443 --cacert ca
Works, authed as system:anonymous

curl https://10.154.0.2:5443 --cacert ca --cert crt --key=key
Authed as the administrator

14. External APIServer access

k edit svc kubernetes
edit as nodeport 

Remember need open firewall

k config view --raw
copy it to local machine conf file 
change server to external ip 
Cert is not valid for this IP.
openssl x509 -in /etc/kubernetes/pki/apiserver.crt
create a hosts entry in hosts file for the extrenal ip address as kubernetes 
in config file change to kubernetes too
Works!

15. NodeRestriction AdmissionController

kube-apiserver --enable-admission-plugins=NodeRestriction

Limits the Node labels a kubelet can modify
Can only modify certain labels (its only node labels, only labels of pods running on the same node)
Secure workload isolation via labels
No one can pretend to be a "secure" node and schedule "secure" pods

Verify it works:
vi /etc/kubernetes/manifests/kube-apiserver.yaml
--enable-admission-plugins=NodeRestriction (should be by default with kubeadm)
On worker node:
k config view
vi /etc/kubernetes/kubelet.conf - config for kubelet-apiserver communication
export KUBECONFIG=/etc/kubernetes/kubelet.conf - now its our kubectl config 
k get ns - no permissions (user system:node:nodename)
k get node - works 
k label node cks-master cks/test=yes - forbidden 
k label node cks-worker cks/test=yes - works 
Restricted labels we can't even set for ourselves
k label node cks-worker node-restriction.kubernetes.io/test=yes - not allowed to

16. Wrap up of auth

Outside -> API
Pod -> API
Node -> API

Anonymous access
Insecure access
Certificates

17. Secrets

ETCDCTL_API=3 etcdctl ...certs... get /registry/secrets/default/secret2

ETCD ENCRYPTION

Create an EncryptionConfiguration

apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - identity: {} kind of deafult provider, nothing is encrypted - plain text
    - aesgcm: encryption method 
        keys:
      - name: key1
        secret: adsfhljksadgh==
      - name: key2
        secret: adfhljkhdsf==
    - aescbc:
        keys:
        - name: key1
          secret: adfhlkjh==
        - name: key2
          secret: adksjfhd==

pass it as an argument to the APIServer
--encryption-provider-config - path to file ^

Encryption config (provider section) works in order!
First one is used on new resource save.

Might do resoruces: - secrets, first aesgcm, later identity as fallback

Need a provider to read! That's why leave identity: {} !!!

Encrypt all after change:
kubectl get secrets --all-namespaces -o json | kubectl replace -f -

Decrypt all: first set provider, second encrypted and recreate all like previously.

18. Container sandboxes

Sandboxes:
- more resources needed
- might be better for smaller containers
- not good for syscall heavy workloads

Contact kernel from the container:
k exec pod -it -- bash
uname -r 
on master the same
both these commands executed uname syscall, returned actual version

strace uname -r - will show which syscalls the process makes

Linux Kernel Dirty Cow 

OCI
kubelet --container-runtime {string}
kubelet --container-runtime-endpoint {string}

Kata containers

Additional isolation with a lightweight VM and individual kernels

- strong separation layer
- runs every container in its own private VM (hypervisor based)
- QEMU as default (needs virtualisation, like nested virtualisation in cloud - might not be asy to configur if already in a VM)

gVisor

user-space kernel for containers

- adds another layer of separation
- not hypervisor/VM based
- simulates kernel syscalls with limited functionality
- runs in userspace separated from Linux Kernel
- runtime called runsc

It's a kernel in golang that accepts syscalls and transforms them to the real kernel

Create and use RuntimeClasses for runsc (gvisor)

RuntimeClass is a k8s resource

apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc

in pod:

spec:
  runtimeClassName: gvisor

scp gvisor-install.sh node01:/root

ssh node01
    sh gvisor-install.sh
    service kubelet status

19. OS Level Security Domains

Security Contexts
Define privilege and access control for Pod/Container
- userID and groupID
- run privileged/unprivileged
- linux capabilities
- ...

spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    this is passed down to all containers
  containers:
    - name: asfafg
    securityContext:
      runAsUser: 0

command id for checking

Force container to run as non-root

vi pod.yaml

spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
  containers:
  -
    securityContext:
      runAsNonRoot: true - container is not allowed to run as root user

Privileged containers
- default containers run unprivileged
- possible to run as pribileged to:
  - access all devices
  - run docker daemon inside container
  - docker run --privileged
- means that container user 0 = host user 0

Enable privileged mode and test using sysctl (set kernel params at runtime if have privileges)

in pod:
enable security context again 1000,3000
run as runAsNonRoot

k exec -it pod -- sh
sysctl kernel.hostname=attacker - won't work 

in pod:
delete old stuff
containers:
  securityContext:
    privileged: true
hostname will work

PrivilegeEscalation

AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process

By default it's allowed.

securityContext:
  allowPrivilegeEscalation: false

/proc/1/status - NoNewPrivs = 1

Pod SecurityPolicies
- Cluster-level resources
- Control under which security conditions a pod has to run

We create a policy and with this policy we kind of set a base all the pods in the cluster have to follow.
Otherwise they won't be created. It's actually an AdmissionController, we enable it and all pods go through
this on creation.

PodSecurityPolicy controls what has been defined in the Pod (eg. SecurityContext)

What can we set:
privileged - running of privileged containers
hostPID, hostIPC - usage of host namespaces
hostNetwork, hostPorts - usage of host networking and ports 
volumes- usage of volume types
allowedHostPaths - usage of the host filesystem
allowedFlexVolumes - allow specific FlexVolume drivers
fsGroup - allocating an FSGroup that owns the pod's volumes
readOnlyRootFilesystem - requiring the use of a read only root filesystem

--enable-admission-plugins=PodSecurityPolicy

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - '*'

Deployment now doesn't work, but k creat pod does.
Because deployment creates the pod - deployment account doesn't see policy.

Fix serviceaccount

k create role psp-access --verb=use --resource=podsecuritypolicies 
k create rolebinding psp-access --role=psp-access --serviceaccount=default:default

Works now!

20. mTLS

- mutual authentication
- two-way (bilateral) authentication 
- two parties authenticating each other at the same time 

To proxy, create an initContainer that needs NET_ADMIN capability

k run app --image=bash --command -oyaml --dry-run=client > file.yml -- sh -c 'ping google.com'

containers:
- bash cont 
- name: proxy
  image: ubuntu
  command:
  - sh
  - c
  - 'apt-get update && apt-get install iptables -y && iptables -L && sleep 1d' (hacky)
  securityContext:
    capabilities:
      add: ["NET_ADMIN"]

21. OPA

General-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack
- not k8s specific
- rego language
- works with json/yaml
- in k8s uses AdmissionControllers
- doesn't know concepts like pods/deployments

OPA Gatekeeper provides K8s CRDs 

Constraint template: (search for necesary labels)
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: K8sRequiredLabels

Constraint: (pods must have labels X)
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: pod-must-have-gk 

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: ns-must-have-gk

Install OPA Gatekeeper

apiserver: --enable-admission-plugins=NodeRestriction (no others)
k create -f https://blablabal

In k8s docs: DYNAMIC ADMISSION CONTROL (admission webhooks)

OPA Gatekeeper creates custom webhooks - every pod creation passes through this webhooks.
There are 2 ways - validation admission webhook (only validate pod spec - apply/deny)
Mutating admission webhook - mutates a pod.
Gatekeeper works with a validating one.

Deny All - Approve All Policy 

k get crd (custom resource definitions)

k get constrainttemplates 

alwaysdeny_template.yml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8salwaysdeny
spec:
  crd:
    spec:
      names:
        kind: K8sAlwaysDeny 
      validation:
        #Schema for the 'parameters' field 
        openAPIV3Schema:
          properties:
            message:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8salwaysdeny

        violation[{"msg": msg}] {
          1 > 0
          msg := input.parameters.message
        }

all_pod_always_deny.yml 
apiVersion:constraints.gatekeeper.sh/v1beta1
kind: K8sAlwaysDeny
metadata:
  name: pod-always-deny 
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
  parameters:
    message: "ACCESS DENIED!"

k get K8sAlwaysDeny

k run --image=nginx - ACCESS DENIED!

k describe K8sAlwaysDeny pod-always-deny

Change it to approve all:

vi template
change 1 > 0 to 1 > 2

All NSes created need to have label "cks"

apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels 
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          properties:
            labels:
              type: array
              items: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        violation [{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("you must provide labels: %v, [missing]")
        }

all_pods_must_have_cks_label.yml

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: pod-must-have-cks
spec:
  math:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
  parameters:
    labels: ["cks"]

all_ns_must_have_cks_label.yml

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: ns-must-have-cks
spec:
  math:
    kinds:
      - apiGroups: [""]
        kinds: ["Namespace"]
  parameters:
    labels: ["cks"]

Enforce deployment replica count

apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sminreplicacount 
spec:
  crd:
    spec:
      names:
        kind: K8sMinReplicaCount 
      validation:
        openAPIV3Schema:
          properties:
            min:
              type: integer
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sminreplicacount
        
        violation[{"msg": msg, "details": {"missing_replicas": missing_replicas}}]
          provided := input.review.object.spec.replicas 
          required := input.parameters.min
          missing := required - provided 
          missing > 0
          msg := sprintf("you must provide %v more replicas", [missing_replicas])
        }

        apiVersion: constraints.gatekeeper.sh/v1beta1
        kind: K8sMinReplicaCount
        metadata:
          name: deployment-must-have-min-replicas
        spec:
          match:
            kinds:
              - apiGroups: ["apps"]
                kinds: ["Deployment"]
          parameters:
            min: 2