DOCS:

kubernetes.io/docs 
kubernetes.io/blog 

github.com/aquasecurity/trivy
falco.org/docs 
gitlab.com/apparmor/apparmor/-/wikis/Documentation

1. NetworkPolicies

deafault-deny-np.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: nptest
spec:
  podSelector: {}
  policyTypes:
- Ingress
- Egress

NetPols are additive - have a default deny, add rules allowing with others.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: nginx-ingress 
  namespace: nptest 
spec:
  podSelector:
    matchLabels:
      app: nginx 
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
      matchLabels:
        project: test
      podSelector:
        matchLabels:
          app: client
    ports:
    - protocol: TCP
      port: 80

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: nginx-ingress 
  namespace: nptest 
spec:
  podSelector:
    matchLabels:
      app: client 
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
      matchLabels:
        project: test
      podSelector:
        matchLabels:
          app: nginx
    ports:
    - protocol: TCP
      port: 80

2. CIS Benchmark

k create -f both kube-bench files (control and worker job)

k log kube-bench-master > kube-bench-master.log
k log kube-bench-worker > kube-bench-worker.log 

/var/lib/kubelet/config.yaml 

3. Ingress TLS

user -> https -> ingress -> http -> service

openssl req -nodes -new -x509 -keyout tls-ingress.key -out tls-ingress.crt -subj "/CN=ingress.test" 

vi ingress-tls-secret.yaml
apiVersion: v1
kind: Secret 
type: kubernetes.io/tls 
metadata:
  name: ingress-tls
  namespace: ingresstest
data:
  tls.crt: |
    <base64-encoded cert data from tls-ingress.crt>
  tls.key: |
    <base64-encoded key data from tls-ingress.key>

tls-ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
  namespace: ingresstest
spec:
  tls:
  - hosts:
    - ingress.test 
    secretName: ingress-tls
  rules:
  - host: ingress.test
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ingresstest-nginx-svc 
            port:
              number: 80

4. Attack surfaces (ports, gui tools, opportunities)

Securing Node Endpoints

Be aware of what ports your K8s nodes are using. Use network segmentation/firewalls to keep them safe!

6443 kube-apiserver
2379-2380 etcd
10250 kubelet API 
10251 kube-scheduler
10252 kube-controller-manager

Worker node listening ports:
10250 kubelet API
30000-32767 NodePort Services

Secure the GUI dashboard!
RBAC, maybe network segmentation, firewalls.

5. Verifying Kubernetes binaries

kubectl version --short --client

curl -LO "https://dl.k8s.io/v1.20.1/bin/linux/amd64/kubectl.sha256"

echo "$(<kubectl.sha256) /usr/bin/kubectl" | sha256sum --check

6. ServiceAccounts

If a container is comproimsed, an attacker can use it to acces K8s API. Only give necessary permissions to SAs.

Examine existing RoleBindings and ClusterRoleBindings to determine what permissions a SA has.

Design your RBAC setup in such a way that service accounts don't have unnecessary permissions.

You can bind multiple roles to an account. Keep roles separate, don't overload them with permissions.

You can bind CRs with RBs.

7. The Kubernetes API 

Limit user account permissions.
Limit network access to the KubeAPI.

8. Kubernetes updates

Keep K8s up to date.

Appr 1 year / 3 minor versions of patch support.

9. Host OS Security

Containers use OS namespaces to isolathe themselves from other containers and the host.

Host Namespaces | Container Namespaces - separation

You can configure pods to use the Host Namespace. Never do it unnecessarily.
spec:
  hostIPC: true      |
  hostNetwork: true  | all default to false
  hostPID: true      |

Privileged mode - allowes containers to access host-level resources and capabilities
                  much like a non-container process running directly on the host.

spec:
  containers:
    - securityContext:
        privileged: true

10. IAM Roles

Containers may be able to access IAM credentials.
Use principle of least privilege. 
If K8s doesn't use IAM, block access to (for EC2, IP address 169.254.169.254)

11. Network-Level Security 

Limit access to the cluster network from outside of the cluster.

By default, anyone who can access the cluster network, can communicate with all Pods and Svcs in the cluster.

When possible, limit access to the cluster network from outside. 

12. AppArmor

Linux Security kernel module. Provides granular access control for programs running on Linux systems. Use AppArmor to control
and limit what a program can do within the host OS.

Complain mode - report on what a program is doing. Use for discovery of behavior.
Enforce mode - actively prevent program from doing anything not allowed.

Need profile on all nodes!

sudo apparmor_parser /path/to/file
-C for complain mode

metadata:
  annotations:
    container.apparmor.security.beta.kubernetes.io/nginx: localhost/k8s-deny-write

sudo vi /etc/apparmor.d/deny-write
sudo apparmor_parser /etc/apparmor.d/deny-write

13. securityContexts

A portion of the Pod and container spec that allows to provide special security and access control settings
at the Pod and container level.

apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  securityContext:
    runAsUser: 1000
  containers:
  - name: nginx
    image: nginx
    securityContext:
      runAsUser: 2000
      allowPrivilegeEscalation: false

14. PodSecurityPolicies

PSPs allow cluster admins to control what security-related configs Pods are allowed to run with.
Use them to automatically enforce desired security configurations within the cluster.

PodSecurityPolicy:
X Privileged container mode 
V Allow privilege escalation 
X Run as root
V Allowed volume types 
X Access host network 

They can also change pods by providing default for certain values.

PodSecurityPolicy:
X Allow privilege escalation 
V Must run as user 1337

If user didn't provide a user - will be 1337.
If they provided the user, Pod would get rejected. 

What they can control:
- privileged mode
- host namespaces
- volumes
- allowedHostPath (specify allowed paths)
- runAsUser/runAsGroup

PSPs are being deprecated!!!

Turn these on with admission controller:
--enable-admission-plugins=PodSecurityPolicy

A Pod must satisfy at least one PSP to be allowed. If you enable PSP without any policy, no Pods allowed!!!

apiVersion: policy/v1beta1
kind: PodSecurityPolicy 
metadata:
  name: my-psp 
spec:
  privileged: false 
  runAsUser:
    rule: RunAsAny

Now authorize the use of the policy:

apiVersion: rbac.authorization.k8s.io/v1 
kind: ClusterRole 
metadata:
  name: cr-use-psp 
rulse:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs: ['use']
  resourceNames:
  - my-psp

For a user to use a PSP in their Pods, they must be authorized to use the policy via RBAC.
The use verb in a Role or ClusterRole allows a user to use PSP.
Every new Pod must be allowed by at least one policy which the user is authorized to use. If not, won't create Pods!

Two ways of authorizing policies:
USER:
- The user creating the Pod has access to use the Policy.
- Control which users can create Pods according to which policies.
- Doesn't work for Pods that are not created directly by users (think Deployments etc.).

SA:
- Pod's SA has access to the policy.
- Works with indirectly-created Pods.
- Preferred method in most cases.

vi kube-apiserver.yaml 

--enable-admission-plugins=NodeRestriction,PodSecurityPolicy 

vi psp-nonpriv.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy 
metadata:
  name: psp-nonpriv
spec:
  privileged: false 
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny 
  supplementalGroups:
    rule: RunAsAny 
  volumes:
  - configMap
  - donwardApi
  - emptyDir
  - persistentVolumeClaim
  - secret
  - projected

k create ns psp-test 
k create sa psp-test-sa -n psp-test

vi cr-use-psp-psp-nopriv.yaml 

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cr-use-psp-psp-nonpriv
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs: ['use']
  resourceNames:
  - psp-nonpriv

RoleBind it.

vi pod-psp-test.yaml 

spec:
  serviceAccountName: psp-test-sa 

15. OPA Gatekeeper

OPA Gatekeeper allows to enforce highly-customizable policies on any kind of k8s object at creation time.
Policies are defined using the OPA Constraint Framework.

Examples:
- image repos (imgs only come from pre-approved repos)
- resource limits (all Pods must specify resource limits)
- labels (all deployments must include certain informational labels)

OPA Gatekeeper Constraint:
All Deployments must have a contact label listing the name  of the user who triggered the Deployment.

CONSTRAINT TEMPLATE 
Defines the schema and the Rego logic that will enforce that constraint.

CONSTRAINT
Attaches the logic in a Constraint Template to incoming k8s objects alongside any parameters defined in the template.

TEMPLATE = NEW KIND

apiVersion: constraints.gatekeeper.sh/v1beta1 
kind: K8sRequiredLabels
metadata:
  name: dep-must-have-contact
spec:
  match:
    kinds:
    - apiGroups: [""]
      kinds: ["Deployment"]
  parameters:
    labels: ["contact"]

16. Secrets

How to get secret data?

k get -oyaml <secret>
base64 --decode

17. RuntimeSandboxes

CR Sandbox is a specialized container runtime providing extra layers of process isolation and greater security.

Use cases:
- untrusted workloads
- small and simple workloads (no need for direct host access, don't mind performance tradeoffs)
- multi-tenant environments (eg. customers can run stuff in the cluster)

Usually comes at the cost of performance.

gVisor/runsc 

gVIsor - Linux Application Kernel that runs in the host OS, offering an additional layer of isolation between host OS and containers.
Something betweeen containerization and a VM.
runsc - OCI-compliant container runtime that integrates gVisor with apps like K8s.

Kata Containers 

Provide an additional layer of isolation by transparently running containers inside lightweight VMs.

Build a Sandbox

- install gVisor Runtime
- configure containerd to interact with runsc
- create a RuntimeClass to designate which Pods need to use the sandboxed runtime

ON ALL NODES
Install gVisor (curl GPG, apt-key add, add-apt, install runsc)
vi /etc/containerd/config.toml
disabled_plugins = ["io.containerd.internal.v1.restart"]
[plugins]
Add runsc block here, under runc config
Find Linux plugin (runtime.v1.linux), shim_debug = true 
systemctl restart containerd 

vi runsc-sandbox.yaml

apiVersion: node.k8s.io/v1
kind: RuntimeClass 
metadata:
  name: runsc-sandbox
handler: runsc 

in Pod:
spec:
  runtimeClassName: runsc-sandbox

18. mTLS and certificates

mTLS - both communicating parties fully authenticate each other and all communications are encrypted

Kubernetes API - allows to obtain certs which you can use un your applications.
Certificate Authority - certificates provided by the API will be generated from a central CA, which can be used for trust purposes.
Programmatic Certificates - can obtain certs programatically using the API.

CertificateSigningRequest - requestor creates a CSR object to request a new cet 
Approve/Deny - CSR can be approved or denied 
RBAC - permissions related to cert signing can be managed via RBAC 

Create a CSR
sudo apt install -y golang-cfssl
huge cfssl genkey command
cat sercer.csr | base64 

vi tls-svc-csr.yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: tls-svc-csr
spec:
  request: |
    <base64stuff>
  signerName: kubernetes.io/kubelet-serving
  usages:
  - digital signature
  - key encipherment
  - server auth

k certificate approve tls-svc-csr

k get csr tls-svc-csr -oyaml
or
k get csr tls-svc-csr -o jsonpath='{.status.certificate}'

and | base64 --decode

19. Images

Software Vulnerability - flaw or weakness in a piece of software that can be used by an attacker.
When possible, use images that contain up-to-date software with security patches.

Some images may not be well-designed, and may contain additional, unneeded software.
Additional software comes with additional risk of vulnerabilities.
Try to minimize the amount of unnecessary software in the images you use.

Compromised Images - attackers may purposefully create images that contain malicious software.
Trusted sources!!!

20. Whitelisting Registries

One way to restrict image registries is the OPA Gatekeeper.

Remember to create the template for both regular and init containers!

apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: whitelist-dockerhub
spec:
  match:
    kinds:
    - apiGroups: [""]
      kinds: ["Pod"]
  parameters:
    repos:
    - "docker.io"

21. Image Validation

Images can be signed using a hash generated from the unique contents of the image.
Signatures can be used to verify that the contents of the image haven't been tampered with.

Supply a SHA-256 hash to validate an image.
image: busybox:1.33.1@sha256:12451937865981764981264dsf53

22. Static Analysis

Static Analysis - looking at source code or config to identify potential security issues.
One way to harden the security of your images is to analyze the Dockerfiles used to create them. 

Things to look for:
- USER root/0 (FINAL ONE and also default if none seen)
- :latest tag in FROMs
- unnecessary software
- sensitive data - no sensitive data like passwords, API keys in the image. Instead, Secrets.

YAML Analysis:
- host namespaces
- privileged mode
- :latest tag
- running as root/0

23. Vulnerability Scanning

Trivy - CLI tool to scan container imgs for vulnerabilities.

trivy image nginx:1.14.1
trivy nginx:1.14.1 - older versions

24. Vulnerability Scanning with an Admission Controller 

Admission Controllers 

Intercept requests to the K8s API.
They can approve, deny or modify the request before changes are made.

ImagePolicyWebhook controller
Sends a request to an external webhook containing information about the image being used.
The webhook can approve or deny the creation of the workload based on the image.

Setting up an image scanner
In order to scan incoming images using ImagePolicyWebhook, we need an app that can receive the webhook requests 
and perform the image scanning.

The ImagePolicyWebhook admission controller sends a JSON request to an external service.
Ext service provides a JSON response indicating whether the images are allowed or disallowed.

Configuring the ImagePolicyWebhook Admission Controller

sudo mkdir /etc/kubernetes/admission-control 
External service uses a self-signed cert.
sudo wget -O /etc/kubernetes/admission-control/imagepolicywebhook-ca.crt https://link-from-repo
Same with api-server-client.crt and api-server-client.key 

sudo vi /etc/kubernetes/admission-control/admission-control.conf
apiVersion: apiserver.config.k8s.io/v1 
kind: AdmissionConfiguration
plugins:
- name: ImagePolicyWebhook
NOW EITHER:
  path: /path/to/confing.conf 
OR:
  configuration:
    imagePolicy:
      kubeConfigFile: /etc/kubernetes/admission-control/imagepolicywebhook_backend.kubeconfig 
      allowTTL: 50
      denyTTL: 50
      retryBackoff: 500
      defaultAllow: true (if false, if external service is down - deny)

vi /etc/kubernetes/admission-control/imagepolicywebhook_backend.kubeconfig
apiVersion: v1 
kind: Config 
clusters:
- name: trivy-k8s-webhook
  cluster:
    certificate-authority: /etc/kubernetes/admission-control/imagepolicywebhook-ca.crt
    server: https://acg.trivy.k8s.webhook:8090/scan !!!!!! ALWAYS HTTPS !!!!!!
contexts:
- name: trivy-k8s-webhook
  context:
    cluster: trivy-k8s-webhook
    user: api-server
current-context: trivy-k8s-webhook
preferences: {}
users:
- name: api-server
  user:
    client-certificate: /etc/kubenetes/admission-control/api-server-client.crt 
    client-key: /etc/kubenetes/admission-control/api-server-client.key 

vi kube-apiserver.yaml
--enable-admission-plugins=NodeRestriction,ImagePolicyWebhook
--admission-control-config-file=/etc/kubernetes/admission-control/admission-control.conf 
Set up volume mounts!

25. Behavioral Analytics

Behavioral Analytics - process of observing what is going on in the system and identifying abnormal and potentially malicious events.

Falco - open-source project created by Sysdig.
Monitors system calls at runtime and alerts on any suspicious activity based upon configurable rules.

Examples:
- privilege escalation
- file access (eg. accessing /usr/bin, /)
- binaries (eg. opening a shell)

Different ways to run Falco - command line, service, web server etc.

CLI:
-r <file> - supply a custom rules file 
-M <seconds> - run Falco for a set number of seconds
falco -r rules.yml -M 45

A Falco rule defines a set of conditions that will trigger an alert. Defined in YAMLs.

- rule: spawned_process_in_test_container
  desc: A process was spawned in the test container.
  condition: container.name = "falco-test" and evt.type = execve
  output: "%evt.time,%user.uid,%proc.name" # falco --list helps here
  priority: WARNING

Falco needs to be installed on the worker nodes.
GPG, repo etc
apt install falco

Now on controlplane
Create a test pod falco-test that cats /etc/shadow

vi falco-rules.yaml 
- rule: spawned_process_in_test_container
  desc: A process was spawne in the test container.
  condition: container.name = "falco-test" and evt.type = execve
  output: "%evt.time,%user.uid,%proc.name,%container.id,%container.name"
  priority: WARNING

sudo falco -r falco-rules.yml -M 45

26. Immutable Containers

Container immutability - (stateless) do not change during lifetime. Instead, replaced with new containers.
Often means that the container fs remains static, and the container doesn't depend on 
non-immutable host resources that require privileged access.

Immutability has security benefits:
- attacker can't donwload malicious software or tools, or alter the container's runtime code

Best practices:
- avoid elevated privileges (don't do securityContext.privileged: true)
- host namespaces (like hostNetwork: true)
- securityContext.allowPrivilegeEscalation: true - allows a container to gain more privileges than the parent process
- securityContext.runAsUser: root/0 

Immutable containers cannot change code at runtime. Don't allow to write to its own fs.
securityContext:
  readOnlyRootFilesystem: true
Use emptyDirs.

27. Audit Logging

Audit Logs - a chronological record of actions performed through the Kube API.
Useful for seeing what's going on in the cluster in real time, or for examining what happened in the cluster
after the fact (post-mortem analysis).

Audit Policy includes a set of rules that determine which events are logged and how detailed the logs are.
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: None #how detailed the rule's logs are. Can be: None, Metadata, Request, RequestResponse
  resources: #matches object types with the applicable rule 
  - group: ""
    resources: ["pods", "services"]
  namespaces: ["test"] #optional, limits to namespaces

kube-apiserver.yaml
--audit-policy-file - audit policy config file 
--audit-log-path - location of log output files
--audit-log-maxage - number of days to keep old log files 
--audit-log-maxbackup - number of old log files to keep 

apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# Log changes to Namespaces at the RequestResponse level.
- level: RequestResponse
  resources:
  - group: ""
    resources: ["namespaces"]

# Log pod changes in the audit-test Namespace at Request level.
- level: Request
  resources:
  - group: ""
    resources: ["pods"]
  namespaces: ["audit-test"]

# Log all ConfigMap and Secret changes at the Metadata level.
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]

# Catch-all - Log all requests at the metadata level.
- level: Metadata

kube-apiserver.yaml
- --audit-policy-file=/etc/kubernetes/audit-policy.yaml 
- --audit-log-path=/var/log/k8s-audit/k8s-audit.log
- --audit-log-maxage=30
- --audit-log-maxbackup=10
VolumeMounts and Volumes