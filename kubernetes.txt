kubectl scale
kubectl rollout status (what's going on wihth rolling update)
kubectl set image deployment/my-deployment nginx=nginx:1.19.0 --record
-- record - records rolling deployment, easy to roll back
kubectl rollout history deployment.v1.apps/my-deployment
kubectl rollout undo deployment.v1.apps/my-deployment --to-revision=1 (rev from rollout history)
without --to-revision - one back

pod-ip-address.namespace-name.pod.cluster.local
192-168-10-100.default.pod.cluster.local

three dashes!

NetworkPolicy
By default pods are completely open. Any network policy selects pods - pod becomes isolated

Components:
podSelector - to which pods in namespace policy applies. Uses labels
spec:
  podSelector:
    matchLabels:
      role: db

from - selects ingress traffic to be allowed
to - selects egress traffic to be allowed
spec:
  ingress:
    - from:
  egress:
    - to:

examples:
spec:
  ingress:
    - from:
      - podSelector:
        matchLabels:
          app: db
spec:
  ingress:
    - from:
      - namespaceSelector:
        matchLabels:
          app: db

spec:
  ingress:
    - from:
      - ipBlock:
        cidr: 172.17.20.0/12

ports - one/more ports that will allow traffic

spec:
  ingress:
    - from:
      ports:
        - protocol: TCP
          port: 80

Traffic has to match pod/ns/cidr AND port

NodeSelector/NodeName

Services

Endpoints!!!

port: 80 - service listens on this port
targetPort: 80 - service routes to this port on the pod
nodePort: 30080

kubectl get endpoints svc-clusterip

Services also get DNS names.
FQDN:
service-name.namespace-name.svc.cluster-domain.example (cluster.local)
If same ns: my-Service

ingress

external -> ingress -> Service
Need ingress controller

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - http:
    paths:
    - path: /somepath
      pathType: Prefix
      backend:
        service:
          name: my-service
          port:
            number: 80
Or route to named ports:
IN SERVICE:
ports:
  - name: web (ingress will look at the service port, auto change)
    protocol: TCP
    port: 80
    targetPort: 8080
IN INGRESS:
backend:
  service:
    name: svc-clusterip
    port:
      name: web

VOLUMES:
1. Create Persistent Volume
2. Create Pers. Vol. Claim
3. In pod add Volume (PVC)
4. In container add Volume Mount

Pod:
spec:
  containers:
  - name: busybox
    image: busybox
    volumeMounts:
    - name: my-volume
      mountPath: /
  volumes:
  - name: my-volume
    hostPath:
      path: /data

You can mount same volume to many containers - great way of interaction.

Common volume types:
hostPath - directory on the node
emptyDir - temporary, dynamically created, useful for simply sharing data between containers on a pod

Persistent volumes:

kind: PersistentVolume
apiVersion: v1
metadata:
  name: my-pv
spec:
  storageClassName: localdisk
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /var/output

Storage Class:

apiVersion: storage.k8s.io/v1
kind: storageClass
metadata:
  name: slow
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true (default false)

For PersistentVolume
persistentVolumeReclaimPolicy: Recycle
Retain - keeps all data. Manually clean and prepare to reuse
Delete - delete RESOURCE automatically (only in cloud storage)
Recycle - delete DATA in storage automatically, allow reuse

Persistent Volume Claim:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: localdisk
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi

PVC automatically binds to a PV after creation if finds a good one

In Pod:

volumes:
- name: pv-storage
  persistentVolumeClaim:
    claimName: my-pvc

You can easily resize PVCs if PV allows and StorageClass allows.

TROUBLESHOOTING:
1. kube-apiserver down:
  - kubectl might not work
  - Connection to the server was refused (if kubeconfig is set up correctly!)
  - make sure docker/containerd and kubelet are up on controlplane nodes
2. node status:
  - check status of nodes kubectl get nodes
  - kubectl describe node (one which is not READY)
3. If node is down, maybe a service is down on the node
  - see kubelet and docker/containerd
  - systemctl status/start/enable kubelet
4. Check system pods in a kubeadm cluster (kube-system namespace)
  - kubectl get po -n kube-system
  - describe the ones that are down

Cluster and node logs:
journalctl -u kubelet
journalctl -u docker

kubectl logs -n kube-system podname

Unhealthy pods:
- kubectl get po
- kubectl describe po
- kubectl exec podname -c containername -- command

kubectl exec busybox --stdin --tty -- /bin/sh

Checking container logs:
kubectl logs podname -c containername

Networking troubleshooting:
1. Networking plugin/infrastructure
2. kube-proxy
3. DNS

nicolaka/netshoot image - networking exploration/troubleshooting tools
command: ['sh', '-c', 'while true; do sleep 5; done']
In netshoot:
  - curl
  - ping
  - nslookup
  - many more (look up github page)

KUBELET DOES NOT RUN AS A POD IN KUBEADM CLUSTER!!!
